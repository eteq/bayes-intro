{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"right\" width=\"30%\"  src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif\">\n",
    "\n",
    "# Crash Intro to Bayesian Statistics\n",
    "\n",
    "### Erik Tollerud\n",
    "[Space Telescope Scince Institute](https://www.stsci.edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What is Bayesian Statistics?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A philosophical interpretation of the concept of probability, based on application of Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So then what is probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A mathematical formalism, traditionally represented using the **Kolmogorov Probability Axioms**.  For a strict definition, see the [all-knowing Wikipedia](https://en.wikipedia.org/wiki/Probability_axioms).  But to summarize:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Probabilities are positive real numbers.\n",
    "2. Probabilities of all the possibilities add up to 1.\n",
    "3. If two events are mutually exclusive, the probability of one or the other is the sum if their individual probabilties.  I.e., $P(A \\cup B) = P(A) + P(B)$\n",
    "\n",
    "An important provable corollary: If they are *not* mutually exclusive: $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$, from which you can conclude $P(A \\cap B) = P(B \\cap A)$.\n",
    "\n",
    "(cheat sheat: $\\cup=$ \"or\", $\\cap=$ \"and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conditional Probability\n",
    "\n",
    "Conditional probabilty is not an axiom or proof, but a *definition*.  Read as \"the probability of A given B\":\n",
    "\n",
    "$P(A|B) \\equiv \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "or more intuitively (to me):\n",
    "\n",
    "$P(A \\cap B) = P(A|B)P(B)$\n",
    "\n",
    "Because that makes it clear that if A and B are independent:\n",
    "\n",
    "$P(A|B) = P(A)$\n",
    "\n",
    "and thus\n",
    "\n",
    "$P(A \\cap B) = P(A)P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Aside:  apparently you can re-cast the axioms using this result in place of 3.  That's probably more natural in some ways? But... Tradition! *(they didn't even ask permission!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayes' Theorem\n",
    "\n",
    "So then where does Bayes come in?  When you want to link a conditional probability to the \"reversed\" version:\n",
    "\n",
    "$P(A|B) \\stackrel{?}{=} {\\rm something} P(B|A)$\n",
    "\n",
    "Intuition says they are connected *somehow*.  But how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That connection comes via a surprisingly simple proof from the axioms:\n",
    "\n",
    "$P(A|B) P(B) = P(A \\cap B)$, $P(B|A) P(A) = P(B \\cap A)$, but the $\\cap$'s have to be equal, so:\n",
    "\n",
    "$P(A|B) P(B) = P(B|A) P(A)$\n",
    "\n",
    "$P(A|B) = P(B|A) \\frac{P(A)}{P(B)}$\n",
    "\n",
    "And the that is Bayes' Theorem (or \"Law\" or \"Rule\" since it's provable from the axioms). m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So what's the big deal?\n",
    "\n",
    "It's all in what you do with this.  If we do a re-labeling and a more practical addition, we start seeing the relevance for science:\n",
    "\n",
    "$P({\\rm Hypothesis}|{\\rm Data, O}) = P({\\rm Data}|{\\rm Hypothesis, O}) \\frac{P({\\rm Hypothesis|O})}{P({\\rm Data}|{\\rm O})}$\n",
    "\n",
    "(where ${\\rm O}=$\"Everything Else you know\")\n",
    "\n",
    "That is, Bayes' Law tells you how do update your scientific beliefs when faced with new data.  Key point: you can't ignore $P({\\rm Hypothesis})$! Thus \"priors\". In this construct, the value of probability is \"degree of certainty\".\n",
    "\n",
    "Interpreted that way, Bayes' Rule gives a guide for infering reality based on data.  Hence \"Bayesian Inference\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bayes Example\n",
    "\n",
    "While most practical scientific problems require more detailed analysis (see later), some respond *directly* to Bayes' Theorem. For example:\n",
    "\n",
    "Do I actually  have COVID if I tested positive, but there's a non-zero false positive rate?\n",
    "\n",
    "$P(C|+)= 1 - P( nC | +) = 1 - P(+ | nC) \\frac{P(nC)}{P(+)}$\n",
    "\n",
    "Take a moment to consider this. How does this help me decide what to do in various false-positive rate scenarios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-.01*(.5/.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-.01*((1-.9)/.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the false positive rate is low, I should quarantine and have a hospital driver ready.\n",
    "* If the false positive rate is higher, I should take a close look at the statistics in the wider population... but also my symptoms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "\n",
    "$\\color{red}{P({\\rm Hypothesis}|{\\rm Data})} = \\color{green}{P({\\rm Data}|{\\rm Hypothesis})} \\frac{\\color{blue}{P({\\rm Hypothesis})}}{\\color{magenta}{P({\\rm Data})}}$\n",
    "\n",
    "<font color='red'>Posterior Probability</font><br>\n",
    "<font color='green'>Likelihood</font><br>\n",
    "<font color='blue'>Prior Probability</font><br>\n",
    "<font color='magenta'>Marginal Likelihood or \"Model Evidence\"</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Frequentism\n",
    "\n",
    "Bayesian statistics is often contrasted with frequentist statistics.  That's not really fair.  They are really very different applications of a set of mathematical laws: frequentism is applying the same rules, but treating the probability as a porportion of repeated events (with an implied concept that \"repeated events\" is a meaningful construct).  Sometimes that looks similar to the Bayesian approach, sometimes it doesn't. Both have uses, but in astro (maybe most science?), the Bayesian approach is often closer to what we are doing in the *big picture*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The original \"practical\" application of frequentist statistics was improving Guiness beer ([seriously!](https://en.wikipedia.org/wiki/Student%27s_t-test)). For a situation where you actually want to do repeated experiments, this can be much simpler to think about.  Similar for classic \"dice rolling\" problems, since you can actually roll real, physical dice!\n",
    "\n",
    "In astronomy, by contrast, we don't get to do repeated experiments.  There's only one universe! (Or... your simulations  are really expensive...)  That makes Bayesian inference a more powerful tool for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For more on this, see an excellent series of posts by [Jake Vanderplas](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Blah, Blah, Blah.  I have numbers and I want to fit them to something.  Tell me what to do!\n",
    "\n",
    "Well too bad.  We have to do one more exposition.\n",
    "\n",
    "$P(Model | Data) = P(Data | Model) \\frac{P(Model)}{P(Data)}$\n",
    "\n",
    "But a \"model\" isn't a thing we fit on it's own. We need the model *parameters*, which we collectively call $\\Theta$ (Why that greek letter? Theta's a good question, I don't know...).  So we apply Bayes' Theorem to *that* under the assumption that the particular model in question is true:\n",
    "\n",
    "\n",
    "$P(\\Theta| D, M) = P(D| \\Theta, M) \\frac{P(\\Theta|M)}{P(D | M)}$\n",
    "\n",
    "So if we want the posterior, the right hand side is then things we can write down for a real, live data set: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Likelihood\n",
    "\n",
    "\n",
    "$P(\\Theta| D, M) = \\color{green}{P(D| \\Theta, M)} \\frac{P(\\Theta|M)}{P(D | M)}$\n",
    "\n",
    "The Likelihood is the same kind of \"likelihood function\" you've likely (get it?) encountered before: the probability of a *specific* data value given a specific parameter value and your assumed model. It has a fixed value for the *data*, whereas the parameter values are to be though of as *variables* (philisophically distinct from the frequentist approach, where the data have to be \"draws\" from a distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prior\n",
    "\n",
    "\n",
    "$P(\\Theta| D, M) = P(D| \\Theta, M) \\frac{\\color{blue}{P(\\Theta|M)}}{P(D | M)}$\n",
    "\n",
    "The prior term encapsulates whatever you knew before the experiment: maybe you think this is nothing, but \"nothing\" is a tricky concept, as numbers have a way of imposing their *own* meaning.  Alternatively, you might know the right order-of-magnitude or know something like \"it can't be negative\".  Put it in here as a probability distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evidence\n",
    "\n",
    "\n",
    "$P(\\Theta| D, M) = P(D| \\Theta, M) \\frac{P(\\Theta|M)}{\\color{magenta}{P(D | M)}}$\n",
    "\n",
    "In the parameter inference formulation, the good news is that we don't worry too much about this term: it's basically a \"normalizing constant\" because the posterior has to be a probability distribution so there's a free parameter to ensure it integrates to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Enough already.  Lets do something!\n",
    "\n",
    "Ok, ok. Do these (in my recommended order, although you probably don't have to):\n",
    "\n",
    "1. [Fitting a gaussian distribution](Session%201%20-%20Gaussian%20Examples.ipynb)\n",
    "2. [The notebook on how I decided 3 sessions is the \"right\" number](bayesian%20session%20counts.ipynb) - this ise a bit more terse than the one above, but reflects a genuine decision question - the data are from a survey of how many sessions is good for a Bayesian intro, but the answer is presented as a Bayesian inference problem.\n",
    "3. [Fitting a line to data using emcee](https://emcee.readthedocs.io/en/stable/tutorials/line/) - note you will need [emcee](https://emcee.readthedocs.io/en/stable/) installed to try this one out. I also strongly suggest instead of downloading the notebook, you read the linked page and type the code into your own notebook, exploring as you do.\n",
    "4. Write your own MCMC! If you want a step-by-step guide, there's a great one [here](https://towardsdatascience.com/from-scratch-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings-in-python-ef21a29e25a).  But you may learn more by instead using a Gaussian data set from #1 with a prose description of MCMC using the Metropolis-Hastings algorithm. Not to sound like a broken record, but [wikipedia may be your friend here](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "Note that this was the barest of bare introductions to Bayesian Inference. If you're looking for something more complete, there are piles of other places to look.  But if you want some of my recommendations:\n",
    "\n",
    "1. [Jayne's \"Probability Theory: The Logic of Science\"](https://bayes.wustl.edu/etj/prob/book.pdf)\n",
    "1. [The Orange Book: \"Statistics, Data Mining, and Machine Learning in Astronomy\"](https://www.amazon.com/Statistics-Mining-Machine-Learning-Astronomy/dp/0691151687)\n",
    "1. [A zillion resources on the internet, choose based on your learning style](https://lmgtfy.com/?q=introduction+to+bayesian+statistics)\n",
    "1. Only partly tongue-in-cheek... [\"Bayesian Probability for Babies\"](https://www.amazon.com/Bayesian-Probability-Babies-Chris-Ferrie/dp/1492680796)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
