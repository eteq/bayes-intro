{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img align=\"right\" width=\"30%\"  src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif\">\n",
    "\n",
    "# Bayesian Inference Part 3: Bayesian Inference Software and Model Selection\n",
    "\n",
    "### Erik Tollerud\n",
    "[Space Telescope Science Institute](https://www.stsci.edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bayes' Theorem Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As a reminder: Bayesian fundamentals were discussed in [Session 1](Session%201.ipynb). Particuarly, a few key pieces of terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$\\color{red}{P({\\rm Hypothesis}|{\\rm Data})} = \\color{green}{P({\\rm Data}|{\\rm Hypothesis})} \\frac{\\color{blue}{P({\\rm Hypothesis})}}{\\color{magenta}{P({\\rm Data})}}$\n",
    "\n",
    "<font color='red'>Posterior Probability</font><br>\n",
    "<font color='green'>Likelihood</font><br>\n",
    "<font color='blue'>Prior Probability</font><br>\n",
    "<font color='magenta'>Marginal Likelihood or \"Model Evidence\"</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "(<font color='magenta' size=\"6\">â†‘</font> This last one in particular will be very relevant this session!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The \"model fitting\" Form\n",
    "\n",
    "$P(Model | Data) = P(Data | Model) \\frac{P(Model)}{P(Data)}$\n",
    "\n",
    "This at first appears to be all we need, but a \"model\" isn't a thing we fit on it's own. With numerical data, we usually have model *parameters*, which we collectively call $\\Theta$ (Why that greek letter? Theta's a good question, I don't know...).  So we apply Bayes' Theorem to *that* under the assumption that the particular model in question is true:\n",
    "\n",
    "\n",
    "$P(\\Theta| D, M) = P(D| \\Theta, M) \\frac{P(\\Theta|M)}{P(D | M)}$\n",
    "\n",
    "In practice, the above is usually what we're usually most interested in when we do Bayesian Inference in astronomy, so we'll focus on that for now (but see the model selection discussion later for a part of the bigger picture).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Model fitting Distributions\n",
    "\n",
    "Also in practice, in astro we are usually thinking about continuous distributions, so these probabilities are also continuous distributions, making this a probability density function statement:\n",
    "\n",
    "$\\mathscr{P}(\\Theta| D_i, M) = \\frac{1}{Z} \\mathscr{L}(X_i, \\Theta) \\pi(\\Theta)$\n",
    "\n",
    "where $Z$ (the evidence) can be treated as simply the  normalization constant to make P a probability distribution. Computing that Z can be tricky though, since it's an integral for a continuous probability distribution. That is, we need to do:\n",
    "\n",
    "$Z = \\int\\mathscr{L}(X_i, \\Theta) \\pi(\\Theta) d\\Theta $\n",
    "\n",
    "and then we can compute $\\mathscr{P}(\\Theta| D_i, M)$, which gives us our \"model fits\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# So why do we need Inference software?\n",
    "\n",
    "In principle we, don't. Back when astronomy \"computers\" were a [group of women working at Harvard](https://en.wikipedia.org/wiki/Harvard_Computers), and honestly well into the digital computer age, analytic techniques were the only practical way to do Bayesian Inference. This tends to revolve around specific statistical distributions where if you choose magical forms for $ \\mathscr{L}$ and $\\pi$, you get a $\\mathscr{P}$ with the same functional form as $\\pi$.  Thus are called \"conjugates\" - i.e. a given likelihood function has a particular \"conjugate prior\". You can see a [nicely formatted table](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions) of these on wikipedia.\n",
    "\n",
    "The trouble with this is threefold: \n",
    "1. These really only work for fairly trivial likelihoods, like pure Gaussians, Binomials, single Poisson processes, etc. So any real physical model is generally right out.\n",
    "2. You are *forced* by math to use a specific form of prior, so if your subjective prior belief doesn't follow that form, you're SOL.\n",
    "3. Historically, [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) would shout at you if you did this, and say you should use frequentist techniques instead because they were more practically applicable (true at the time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Markov Chain Monte Carlo\n",
    "\n",
    "Given the limitations of analytic methods, we need a way to somehow characterize $\\mathscr{P}(\\Theta| D_i, M)$ numerically. Bayesian Inference really came into its own with the recognition that methods developed from statistical mechanics can solve this problem. One key realization is that for parameter distributions you don't necessarily *need* to solve for $Z$. It's independent of all the $\\Theta$s, so it's sufficient to say \"if I get a bunch of samples from this distribution, I can just normalize by dividing by the number of samples\".  It is exactly this problem that Markov Chain Monte Carlo (MCMC) codes are good at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "First we need to decode some of these words:\n",
    "\n",
    "*  Markov Chain: Roughly speaking, this is a list of numbers generated by a process that doesn't care about its history: the \"next\" number only cares about the immediately previous one, not any of those further back.\n",
    "*  Monte Carlo: using random numbers to do some sort of calculation.\n",
    "\n",
    "As you can probably guess, the way astronomers sometimes use this: \"I used MCMC\", \"I MCMCed it\", or \"I did it with that fancy MCMC thingie so it must be right\" (all actual quotes...) is a very incomplete description. MCMC is a broad class of things, and we usually mean a much more limited subset. E.g. \"I did Bayesian Inference using a Metropolis-Hastings-based MCMC code\".  But that doesn't really roll off the tongue so... ðŸ¤·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Metropolis-Hastings\n",
    "\n",
    "In reality the important algorithm here is a specific type of MCMC method - the Metropolis-Hastings algorithm. At its core this means the following algorithm:\n",
    "\n",
    "1. Do a\n",
    "2. Do b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Practicalities of MCMC methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Useful Codes\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Advantages of MCMC\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Disadvantages of MCMC\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Model Selection\n",
    "\n",
    "To address some of these limitations, we're going to talk about another category of algorithms. But before we can do that, we need to introduce a slightly different Bayesian problem, as those algorithms are actually meant to solve *those* problems (and in fact just by chance also turn out to be great Bayesian Inference engines as well): Bayesian Model Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Nested Sampling\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
